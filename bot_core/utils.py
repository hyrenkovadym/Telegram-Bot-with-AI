# bot_core/utils.py
import os
import re
import time
import math
import json
from contextlib import suppress
from typing import Set, Iterable, List, Dict, Any

import openai

from telegram import Update
from telegram.ext import ContextTypes

from .config import (
    OPENAI_API_KEY,
    FREE_MODE,
    KB_DIR,
    KB_INDEX_PATH,
    BLACKLIST_FILE,
    SESSION_TIMEOUT_SEC,
    USE_WEB,
    F_PHONE,
    F_SITE,
)
from .logging_setup import logger

# ======== PDF reader ========
try:
    from pypdf import PdfReader
except Exception:
    PdfReader = None

# ======== HTML fetch (DuckDuckGo) ========
try:
    import requests
    from bs4 import BeautifulSoup
except Exception:
    requests = None
    BeautifulSoup = None

# ======== OpenAI client for embeddings ========
_EMBED_CLIENT = None
if not FREE_MODE and OPENAI_API_KEY:
    _EMBED_CLIENT = openai.OpenAI(api_key=OPENAI_API_KEY)


# ========= PHONE & TEXT UTILS =========
def normalize_phone(phone_raw: str) -> str:
    """
    –ñ–æ—Ä—Å—Ç–∫–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –Ω–æ–º–µ—Ä–∞: –ª–∏—à–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ —Ü–∏—Ñ—Ä–∏ + –º–æ–∂–ª–∏–≤–∏–π –ø–ª—é—Å.
    """
    if not phone_raw:
        return ""
    s = phone_raw.strip()
    plus = "+" if s.startswith("+") else ""
    digits = re.sub(r"\D", "", s)
    return (plus + digits) if digits else ""


def try_normalize_user_phone(text: str) -> str | None:
    """
    "–†–æ–∑—É–º–Ω–∞" –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–µ–ª–µ—Ñ–æ–Ω—ñ–≤ –∑ —Ç–µ–∫—Å—Ç—É:
    - +380XXXXXXXXX
    - 0XXXXXXXXX ‚Üí +380XXXXXXXXX
    - 380XXXXXXXXX ‚Üí +380XXXXXXXXX
    - 12 —Ü–∏—Ñ—Ä ‚Üí +XXXXXXXXXXXX
    """
    if not text:
        return None
    s = text.strip()
    has_plus = s.startswith("+")
    digits = re.sub(r"\D", "", s)
    if not digits:
        return None

    if has_plus:
        return "+" + digits
    if digits.startswith("0") and len(digits) == 10:
        return "+380" + digits[1:]
    if digits.startswith("380"):
        return "+" + digits
    if len(digits) == 12:
        return "+" + digits
    return None


def contains_emoji(text: str) -> bool:
    if not text:
        return False
    return bool(
        re.compile(
            "["  # –¥—ñ–∞–ø–∞–∑–æ–Ω–∏ emoji
            "\U0001F600-\U0001F64F"
            "\U0001F300-\U0001F5FF"
            "\U0001F680-\U0001F6FF"
            "\U0001F1E0-\U0001F1FF"
            "]+",
            flags=re.UNICODE,
        ).search(text)
    )


# ========= BLACKLIST / STAFF NUMBERS =========
# –æ–¥–∏–Ω —Å–ø—ñ–ª—å–Ω–∏–π –∫–µ—à –¥–ª—è "—Å–ø–µ—Ü"-–Ω–æ–º–µ—Ä—ñ–≤ (—É —Ç.—á. —Å–ø—ñ–≤—Ä–æ–±—ñ—Ç–Ω–∏–∫–∏)
_BLACKLIST_NORMALIZED: Set[str] = set()


def _iter_blacklist_lines(path: str) -> Iterable[str]:
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            raw = line.strip()
            if not raw or raw.startswith("#"):
                continue
            yield raw


def reload_blacklist() -> int:
    """
    –ü–µ—Ä–µ—á–∏—Ç—É—î blacklist_phones.txt —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –Ω–æ–º–µ—Ä–∏.
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —è–∫ –¥–ª—è "–Ω–µ –¥–∑–≤–æ–Ω–∏—Ç–∏", —Ç–∞–∫ —ñ –¥–ª—è —Å–ø–µ—Ü-–Ω–æ–º–µ—Ä—ñ–≤ (—Å–ø—ñ–≤—Ä–æ–±—ñ—Ç–Ω–∏–∫–∏).
    """
    global _BLACKLIST_NORMALIZED
    new_set: Set[str] = set()
    for raw in _iter_blacklist_lines(BLACKLIST_FILE):
        norm = normalize_phone(raw)
        if norm:
            new_set.add(norm)
    _BLACKLIST_NORMALIZED = new_set
    logger.info("Blacklist loaded: %s numbers", len(_BLACKLIST_NORMALIZED))
    return len(_BLACKLIST_NORMALIZED)


def is_blacklisted(phone: str) -> bool:
    """
    –ß–∏ —î –Ω–æ–º–µ—Ä —É —Å–ø–∏—Å–∫—É —Å–ø–µ—Ü-–Ω–æ–º–µ—Ä—ñ–≤/blacklist.
    """
    norm = normalize_phone(phone)
    return bool(norm) and (norm in _BLACKLIST_NORMALIZED)


def is_staff_phone(phone: str) -> bool:
    """
    –ù–æ–º–µ—Ä —Å–ø—ñ–≤—Ä–æ–±—ñ—Ç–Ω–∏–∫–∞: –∑–∞—Ä–∞–∑ –ø—Ä–∏—Ä—ñ–≤–Ω—é—î–º–æ –¥–æ –Ω–æ–º–µ—Ä—ñ–≤ —ñ–∑ blacklist_phones.txt.
    –Ø–∫—â–æ –∑–∞—Ö–æ—á–µ—à ‚Äî –ª–µ–≥–∫–æ –≤–∏–Ω–µ—Å–µ–º–æ –≤ –æ–∫—Ä–µ–º–∏–π —Ñ–∞–π–ª staff_phones.txt.
    """
    norm = normalize_phone(phone)
    return bool(norm) and (norm in _BLACKLIST_NORMALIZED)


# ========= DIALOG & SESSION MEMORY =========
def ensure_dialog(context: ContextTypes.DEFAULT_TYPE):
    ud = context.user_data
    ud.setdefault("dialog", [])
    ud.setdefault("last_time", time.time())
    ud.setdefault("phone", "")
    ud.setdefault("first_q_saved", False)
    return ud


def touch_session(context: ContextTypes.DEFAULT_TYPE):
    context.user_data["last_time"] = time.time()


def session_expired(context: ContextTypes.DEFAULT_TYPE) -> bool:
    last = context.user_data.get("last_time", 0)
    return (time.time() - last) > SESSION_TIMEOUT_SEC


def reset_session(context: ContextTypes.DEFAULT_TYPE):
    context.user_data.clear()
    context.user_data.update(
        {
            "dialog": [],
            "last_time": time.time(),
            "phone": "",
            "first_q_saved": False,
        }
    )


def _now() -> float:
    return time.time()


def add_history(context: ContextTypes.DEFAULT_TYPE, role: str, content: str):
    ud = context.user_data
    hist = ud.setdefault("dialog", [])
    hist.append({"role": role, "content": content, "ts": _now()})
    cutoff = _now() - SESSION_TIMEOUT_SEC
    hist[:] = [t for t in hist if t.get("ts", _now()) >= cutoff]
    if len(hist) > 80:
        del hist[: len(hist) - 80]


def last_user_message(context: ContextTypes.DEFAULT_TYPE) -> str | None:
    for turn in reversed(context.user_data.get("dialog", [])):
        if turn.get("role") == "user":
            return turn.get("content", "")
    return None


# ========= SESSION (JobQueue) =========
def schedule_session_expiry(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    –ü–ª–∞–Ω—É—î–º–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Å–µ—Å—ñ—ó —á–µ—Ä–µ–∑ SESSION_TIMEOUT_SEC.
    """
    jq = context.job_queue
    if jq is None:
        logger.warning(
            'JobQueue –≤—ñ–¥—Å—É—Ç–Ω—ñ–π. –í—Å—Ç–∞–Ω–æ–≤–∏: pip install "python-telegram-bot[job-queue]"'
        )
        return

    chat_id = update.effective_chat.id
    old_job = context.chat_data.get("expiry_job")
    if old_job:
        with suppress(Exception):
            old_job.schedule_removal()

    job = jq.run_once(
        end_session_job,
        when=SESSION_TIMEOUT_SEC,
        chat_id=chat_id,
        name=f"expire_{chat_id}",
    )
    context.chat_data["expiry_job"] = job
    context.chat_data["last_time"] = time.time()


async def end_session_job(context: ContextTypes.DEFAULT_TYPE):
    """
    –í–∏–∫–ª–∏–∫–∞—î—Ç—å—Å—è –ø—ñ—Å–ª—è —Ç–∞–π–º-–∞—É—Ç—É —Å–µ—Å—ñ—ó.
    –ß–∏—Å—Ç–∏–º–æ chat_data —Ç–∞ –≤—ñ–¥–ø—Ä–∞–≤–ª—è—î–º–æ –∫—ñ–ª—å–∫–∞ —Å–ª—É–∂–±–æ–≤–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å.
    """
    chat_id = context.job.chat_id
    from .ui import bottom_keyboard, main_menu_keyboard  # –ª–æ–∫–∞–ª—å–Ω–∏–π —ñ–º–ø–æ—Ä—Ç —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ —Ü–∏–∫–ª—ñ–≤

    try:
        await context.bot.send_message(
            chat_id=chat_id,
            text="–î—è–∫—É—é –∑–∞ –∑–∞–ø–∏—Ç–∞–Ω–Ω—è!",
            reply_markup=main_menu_keyboard(),
        )
        await context.bot.send_message(
            chat_id=chat_id,
            text=(
                "–ü—ñ–¥–ø–∏—à—ñ—Ç—å—Å—è –Ω–∞ –Ω–∞—à—ñ —Å–æ—Ü–º–µ—Ä–µ–∂—ñ, —â–æ–± –±—É—Ç–∏ –≤ –∫—É—Ä—Å—ñ –Ω–æ–≤–∏–Ω —ñ –∫–æ—Ä–∏—Å–Ω–∏—Ö –ø–æ—Ä–∞–¥.\n"
                f"–¢–µ–ª–µ—Ñ–æ–Ω –ø—ñ–¥—Ç—Ä–∏–º–∫–∏: {F_PHONE}\n–í–µ–±—Å–∞–π—Ç: https://{F_SITE}\n\n"
                "–Ø –≤–∞—à –ø–æ–º—ñ—á–Ω–∏–∫ FRENDT. –ó–∞–¥–∞–≤–∞–π—Ç–µ —Å–≤–æ—î –ø–∏—Ç–∞–Ω–Ω—è üëá"
            ),
            reply_markup=main_menu_keyboard(),
        )
        await context.bot.send_message(
            chat_id=chat_id,
            text="–ì–æ—Ç–æ–≤–∏–π –¥–æ–ø–æ–º–æ–≥—Ç–∏.",
            reply_markup=bottom_keyboard(context, tg_user_id=str(chat_id)),
        )
    except Exception as e:
        logger.warning(
            "–ù–µ –≤–¥–∞–ª–æ—Å—è –Ω–∞–¥—ñ—Å–ª–∞—Ç–∏ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –ø—Ä–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—è —Å–µ—Å—ñ—ó: %s", e
        )

    chat_data = context.application.chat_data.get(chat_id)
    if isinstance(chat_data, dict):
        chat_data.clear()


# ========= KB BUILDING & SEARCH =========
def _chunk_text(txt: str, chunk_size: int = 900, overlap: int = 120) -> List[str]:
    txt = re.sub(r"[ \t]+", " ", txt)
    txt = re.sub(r"\n{3,}", "\n\n", txt).strip()
    chunks = []
    i = 0
    while i < len(txt):
        chunk = txt[i : i + chunk_size]
        if i + chunk_size < len(txt):
            j = chunk.rfind(". ")
            if j > 300:
                chunk = chunk[: j + 1]
        chunks.append(chunk.strip())
        i += max(len(chunk) - overlap, 1)
    return [c for c in chunks if len(c) >= 120]


def _pdf_to_text(path: str) -> str:
    if PdfReader is None:
        logger.warning("–ø–∞–∫–µ—Ç pypdf –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é PDF: %s", path)
        return ""
    try:
        reader = PdfReader(path)
        return "\n".join([(p.extract_text() or "") for p in reader.pages])
    except Exception as e:
        logger.error("[KB] PDF read fail %s: %s", path, e)
        return ""


def _txt_to_text(path: str) -> str:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        logger.error("[KB] TXT read fail %s: %s", path, e)
        return ""


def _embed_texts(texts: List[str]) -> List[List[float]]:
    if FREE_MODE or _EMBED_CLIENT is None:
        # —É FREE_MODE –µ–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ
        return [[0.0] for _ in texts]
    resp = _EMBED_CLIENT.embeddings.create(
        model="text-embedding-3-small",
        input=texts,
    )
    return [d.embedding for d in resp.data]


def _cosine(a: List[float], b: List[float]) -> float:
    dot = sum(x * y for x, y in zip(a, b))
    na = math.sqrt(sum(x * x for x in a)) or 1e-8
    nb = math.sqrt(sum(y * y for y in b)) or 1e-8
    return dot / (na * nb)


def _tokenize_query(q: str) -> List[str]:
    """
    –†–æ–∑–±–∏–≤–∞—î–º–æ –∑–∞–ø–∏—Ç –Ω–∞ —Å–º–∏—Å–ª–æ–≤—ñ —Ç–æ–∫–µ–Ω–∏:
    - –≤—Å–µ –¥–æ –Ω–∏–∂–Ω—å–æ–≥–æ —Ä–µ–≥—ñ—Å—Ç—Ä—É
    - –∑–∞–ª–∏—à–∞—î–º–æ:
        * —É—Å—ñ —á–∏—Å–ª–∞
        * —Å–ª–æ–≤–∞ –≤—ñ–¥ 4+ —Å–∏–º–≤–æ–ª—ñ–≤ (—É–∫—Ä/–ª–∞—Ç–∏–Ω–∏—Ü—è)
        * –ª–∞—Ç–∏–Ω—Å—å–∫—ñ –∞–±—Ä–µ–≤—ñ–∞—Ç—É—Ä–∏ —Ç–∏–ø—É RTK, GPS (3+ —Å–∏–º–≤–æ–ª–∏)
    """
    q = q.lower()
    raw_tokens = re.findall(r"[a-z–∞-—â—å—é—è—î—ñ—ó“ë0-9]+", q)

    tokens: List[str] = []
    for t in raw_tokens:
        if t.isdigit():
            tokens.append(t)
        elif len(t) >= 4:
            tokens.append(t)
        elif len(t) >= 3 and re.match(r"[a-z0-9]+$", t):
            tokens.append(t)
    return tokens


def kb_build_or_load() -> Dict[str, Any]:
    """
    –ë—É–¥—É—î –∞–±–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î —ñ–Ω–¥–µ–∫—Å –±–∞–∑–∏ –∑–Ω–∞–Ω—å —ñ–∑ KB_DIR —É KB_INDEX_PATH.
    """
    os.makedirs(KB_DIR, exist_ok=True)

    if FREE_MODE:
        logger.info("[KB] FREE_MODE: —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—è –±–µ–∑ OpenAI (—Ç—ñ–ª—å–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤–∏–π –ø–æ—à—É–∫).")

    # –ø—Ä–æ–±—É—î–º–æ –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ —ñ—Å–Ω—É—é—á–∏–π —ñ–Ω–¥–µ–∫—Å
    if os.path.exists(KB_INDEX_PATH):
        try:
            with open(KB_INDEX_PATH, "r", encoding="utf-8") as f:
                idx = json.load(f)
            files_now = []
            for fn in os.listdir(KB_DIR):
                if fn.lower().endswith((".txt", ".pdf")):
                    path = os.path.join(KB_DIR, fn)
                    files_now.append(
                        {"path": path, "mtime": os.path.getmtime(path)}
                    )
            old = {
                (d["path"], round(d.get("mtime", 0), 6))
                for d in idx.get("files", [])
            }
            cur = {
                (d["path"], round(d.get("mtime", 0), 6))
                for d in files_now
            }
            if old == cur and idx.get("chunks"):
                logger.info("[KB] –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —ñ–Ω–¥–µ–∫—Å: %s", KB_INDEX_PATH)
                return idx
        except Exception as e:
            logger.warning("[KB] –ù–µ–º–æ–∂–ª–∏–≤–æ –ø—Ä–æ—á–∏—Ç–∞—Ç–∏ —ñ–Ω–¥–µ–∫—Å (%s). –ü–µ—Ä–µ–±—É–¥–æ–≤—É—é‚Ä¶", e)

    pdf_paths = [
        os.path.join(KB_DIR, fn)
        for fn in os.listdir(KB_DIR)
        if fn.lower().endswith(".pdf")
    ]
    txt_paths = [
        os.path.join(KB_DIR, fn)
        for fn in os.listdir(KB_DIR)
        if fn.lower().endswith(".txt")
    ]

    all_chunks: List[Dict[str, Any]] = []
    for path in txt_paths:
        txt = _txt_to_text(path)
        for i, ch in enumerate(_chunk_text(txt)):
            all_chunks.append(
                {
                    "text": ch,
                    "source": os.path.basename(path),
                    "i": i,
                    "type": "txt",
                }
            )

    for path in pdf_paths:
        txt = _pdf_to_text(path)
        if not txt:
            continue
        for i, ch in enumerate(_chunk_text(txt)):
            all_chunks.append(
                {
                    "text": ch,
                    "source": os.path.basename(path),
                    "i": i,
                    "type": "pdf",
                }
            )

    if not all_chunks:
        logger.warning("[KB] –ü–æ—Ä–æ–∂–Ω—ñ–π –∫–æ–Ω—Ç–µ–Ω—Ç. –ü–æ–∫–ª–∞–¥–∏ .txt –∞–±–æ .pdf —É %s", KB_DIR)
        return {"model": "text-embedding-3-small", "files": [], "chunks": []}

    embeds = _embed_texts([c["text"] for c in all_chunks])
    for c, emb in zip(all_chunks, embeds):
        c["embedding"] = emb

    files_meta = []
    for fn in os.listdir(KB_DIR):
        if fn.lower().endswith((".txt", ".pdf")):
            p = os.path.join(KB_DIR, fn)
            files_meta.append({"path": p, "mtime": os.path.getmtime(p)})

    idx = {
        "model": "text-embedding-3-small",
        "files": files_meta,
        "chunks": all_chunks,
    }
    try:
        with open(KB_INDEX_PATH, "w", encoding="utf-8") as f:
            json.dump(idx, f, ensure_ascii=False)
        logger.info("[KB] –ü–æ–±—É–¥–æ–≤–∞–Ω–æ —ñ–Ω–¥–µ–∫—Å —ñ–∑ %d —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤.", len(all_chunks))
    except Exception as e:
        logger.warning("[KB] –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–±–µ—Ä–µ–≥—Ç–∏ —ñ–Ω–¥–µ–∫—Å: %s", e)
    return idx


_KB_INDEX: Dict[str, Any] = {}


def kb_retrieve_smart(query: str, k: int = 6) -> List[Dict[str, Any]]:
    """
    –†–æ–∑—É–º–Ω–∏–π –ø–æ—à—É–∫ –ø–æ –±–∞–∑—ñ –∑–Ω–∞–Ω—å:
    1) –°–ø–æ—á–∞—Ç–∫—É —Ä–∞–Ω–∂—É—î–º–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏ –∑–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—é –∑–±—ñ–≥—ñ–≤ —Ç–æ–∫–µ–Ω—ñ–≤.
    2) –Ø–∫—â–æ —î –∑–±—ñ–≥–∏ ‚Äî –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ç–æ–ø-k + —â–µ 1‚Äì2 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏ –∑ –Ω–∞–π–≤–∏—â–æ—é —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ—é
       —Å—Ö–æ–∂—ñ—Å—Ç—é.
    3) –Ø–∫—â–æ –±—É–∫–≤–∞–ª—å–Ω–∏—Ö –∑–±—ñ–≥—ñ–≤ –Ω–µ–º–∞—î ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ç—ñ–ª—å–∫–∏ –µ–º–±–µ–¥–¥–∏–Ω–≥–∏.
    """
    if not _KB_INDEX or not _KB_INDEX.get("chunks"):
        return []

    tokens = _tokenize_query(query)
    chunks = _KB_INDEX["chunks"]

    literal_scored: List[tuple[int, Dict[str, Any]]] = []

    if tokens:
        for ch in chunks:
            t = ch["text"].lower()
            hit_count = sum(1 for tok in tokens if tok in t)
            if hit_count > 0:
                literal_scored.append((hit_count, ch))

    if literal_scored:
        literal_scored.sort(key=lambda x: x[0], reverse=True)
        top_literal = [c for _, c in literal_scored[:k]]

        # –¥–æ–¥–∞—î–º–æ 1‚Äì2 –Ω–∞–π–∫—Ä–∞—â—ñ —Å–µ–º–∞–Ω—Ç–∏—á–Ω—ñ –∑–±—ñ–≥–∏
        try:
            q_emb = _embed_texts([query])[0]
        except Exception:
            return top_literal

        scored_sem: List[tuple[float, Dict[str, Any]]] = []
        for ch in chunks:
            sim = _cosine(q_emb, ch.get("embedding", [0.0]))
            scored_sem.append((sim, ch))
        scored_sem.sort(key=lambda x: x[0], reverse=True)

        extra: List[Dict[str, Any]] = []
        for _, ch in scored_sem:
            if ch not in top_literal:
                extra.append(ch)
            if len(extra) >= 2:
                break

        return top_literal + extra

    # —è–∫—â–æ –±—É–∫–≤–∞–ª—å–Ω–∏—Ö –∑–±—ñ–≥—ñ–≤ –Ω–µ–º–∞—î ‚Äî –ª–∏—à–µ –µ–º–±–µ–¥–¥–∏–Ω–≥–∏
    q_emb = _embed_texts([query])[0]
    scored_sem = []
    for ch in chunks:
        sim = _cosine(q_emb, ch.get("embedding", [0.0]))
        scored_sem.append((sim, ch))
    scored_sem.sort(key=lambda x: x[0], reverse=True)
    return [c for _, c in scored_sem[:k]]


def pack_snippets(snips: List[Dict[str, Any]], max_chars: int = 5000) -> str:
    """
    –ü–∞–∫—É—î –∫—ñ–ª—å–∫–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ñ–≤ KB —É –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤–∏–π –±–ª–æ–∫ –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è.
    """
    out: List[str] = []
    total = 0
    for s in snips:
        tag = f"[{s['source']} ‚Ä¢ {s['i']}]"
        block = f"{tag}\n{s['text'].strip()}"
        if total + len(block) > max_chars and out:
            break
        out.append(block)
        total += len(block)
    return "\n\n---\n\n".join(out)


# ========= WEB FALLBACK =========
def fetch_url(url: str, timeout: float = 8.0) -> str:
    if requests is None:
        return ""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
            "Accept-Language": "uk,ru;q=0.9,en;q=0.8",
        }
        r = requests.get(url, headers=headers, timeout=timeout)
        if r.status_code == 200 and r.text:
            return r.text
    except Exception as e:
        logger.debug("fetch_url error %s: %s", url, e)
    return ""


def duckduckgo_search(query: str, n: int = 3) -> List[str]:
    if requests is None or BeautifulSoup is None:
        return []
    q = query.strip()
    url = f"https://duckduckgo.com/html/?q={requests.utils.quote(q)}&kl=ua-uk&kp=1"
    html = fetch_url(url)
    if not html:
        return []
    soup = BeautifulSoup(html, "html.parser")
    links: List[str] = []
    for a in soup.select("a.result__a"):
        href = a.get("href")
        if href and href.startswith("http") and "duckduckgo.com" not in href:
            links.append(href)
        if len(links) >= n:
            break
    return links


def extract_text_from_html(html: str, max_chars: int = 4000) -> str:
    if BeautifulSoup is None:
        return ""
    soup = BeautifulSoup(html, "html.parser")
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()
    text = soup.get_text("\n")
    text = re.sub(r"\n{3,}", "\n\n", text)
    text = re.sub(r"[ \t]+", " ", text)
    return text.strip()[:max_chars]


def build_web_context(query: str, max_pages: int = 3) -> str:
    """
    –§–æ—Ä–º—É—î —Ç–µ–∫—Å—Ç–æ–≤–∏–π "–∫–æ–Ω—Ç–µ–∫—Å—Ç —ñ–∑ –≤–µ–±—É" –¥–ª—è GPT –Ω–∞ –æ—Å–Ω–æ–≤—ñ DuckDuckGo.
    """
    if FREE_MODE or not USE_WEB:
        return ""
    urls = duckduckgo_search(query, n=max_pages)
    chunks: List[str] = []
    for u in urls:
        html = fetch_url(u)
        if not html:
            continue
        plain = extract_text_from_html(html)
        if plain:
            chunks.append(f"[{u}]\n{plain}")
    return "\n\n---\n\n".join(chunks)[:6000]


def clean_plain_text(s: str) -> str:
    """
    –ü—Ä–∏–±–∏—Ä–∞—î Markdown-–≤–∏–¥—ñ–ª–µ–Ω–Ω—è (*...*, _..._, `...`) —ñ –∑–∞–π–≤—ñ –ø–µ—Ä–µ–≤–æ–¥–∏ —Ä—è–¥–∫—ñ–≤.
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ GPT –ø–µ—Ä–µ–¥ –≤—ñ–¥–ø—Ä–∞–≤–∫–æ—é –≤ Telegram.
    """
    if not s:
        return s
    s = re.sub(r"\*{1,3}([^*\n]+)\*{1,3}", r"\1", s)
    s = re.sub(r"_{1,3}([^_\n]+)_{1,3}", r"\1", s)
    s = re.sub(r"`{1,3}", "", s)
    s = re.sub(r"\r\n", "\n", s).strip()
    return s

# ========= LONG TELEGRAM MESSAGES =========
async def send_long_reply(
    update: Update,
    context: ContextTypes.DEFAULT_TYPE,
    text: str,
    reply_markup=None,
    chunk_size: int = 3500,
):
    """
    –ù–∞–¥—Å–∏–ª–∞—î –¥–æ–≤–≥–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤ –∫—ñ–ª—å–∫–∞ –º–µ—Å–µ–¥–∂—ñ–≤, —è–∫—â–æ –≤–æ–Ω–æ –ø–µ—Ä–µ–≤–∏—â—É—î –ª—ñ–º—ñ—Ç.

    - –ö–æ—Ä–∏—Å–Ω–æ –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π GPT —Ç–∞ —Å–µ—Ä–≤—ñ—Å–Ω–æ–≥–æ AI, —â–æ–± –Ω–µ –≤–ø–∏—Ä–∞—Ç–∏—Å—è —É –ª—ñ–º—ñ—Ç Telegram (~4096 —Å–∏–º–≤–æ–ª—ñ–≤).
    - –ü–µ—Ä—à–∞ —á–∞—Å—Ç–∏–Ω–∞ –æ—Ç—Ä–∏–º—É—î reply_markup (–∫–ª–∞–≤—ñ–∞—Ç—É—Ä—É), –Ω–∞—Å—Ç—É–ø–Ω—ñ ‚Äî –±–µ–∑ –Ω–µ—ó.
    """
    if not text:
        return

    s = str(text).strip()
    if not s:
        return

    parts: List[str] = []
    while len(s) > chunk_size:
        # –ü—Ä–∞–≥–Ω–µ–º–æ —Ä—ñ–∑–∞—Ç–∏ –ø–æ "–∫—Ä–∞—Å–∏–≤–∏—Ö" –º–µ–∂–∞—Ö
        cut = s.rfind("\n\n", 0, chunk_size)
        if cut == -1:
            cut = s.rfind("\n", 0, chunk_size)
        if cut == -1:
            cut = s.rfind(". ", 0, chunk_size)
        if cut == -1:
            cut = chunk_size

        chunk = s[:cut].strip()
        if chunk:
            parts.append(chunk)
        s = s[cut:].lstrip()

    if s.strip():
        parts.append(s.strip())

    for i, part in enumerate(parts):
        try:
            if i == 0:
                await update.message.reply_text(part, reply_markup=reply_markup)
            else:
                await update.message.reply_text(part)
        except Exception as e:
            logger.error("send_long_reply error: %s", e)
            break
